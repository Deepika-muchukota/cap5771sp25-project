{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71329042-768d-454e-a51e-de5d92a45b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(self):\n",
    "    \"\"\"Combine data from different sources to create training datasets\"\"\"\n",
    "    \n",
    "    # Create a dataframe to hold features and targets\n",
    "    combined_data = pd.DataFrame()\n",
    "    \n",
    "    # Extract country data\n",
    "    countries = []\n",
    "    anxiety_rates = []\n",
    "    bipolar_rates = []\n",
    "    schizo_rates = []\n",
    "    eating_disorder_rates = []\n",
    "    \n",
    "    # Extract from disorders.csv\n",
    "    if hasattr(self, 'disorders_df'):\n",
    "        for country in self.disorders_df['Entity'].unique():\n",
    "            country_data = self.disorders_df_latest[self.disorders_df_latest['Entity'] == country]\n",
    "            if not country_data.empty:\n",
    "                countries.append(country)\n",
    "                \n",
    "                # Get prevalence rates for each condition\n",
    "                anxiety = country_data['Anxiety'].values[0] if 'Anxiety' in country_data.columns else np.nan\n",
    "                bipolar = country_data['Bipolar'].values[0] if 'Bipolar' in country_data.columns else np.nan\n",
    "                schizo = country_data['Schizophrenia'].values[0] if 'Schizophrenia' in country_data.columns else np.nan\n",
    "                eating = country_data['Eating Disorders'].values[0] if 'Eating Disorders' in country_data.columns else np.nan\n",
    "                \n",
    "                anxiety_rates.append(anxiety)\n",
    "                bipolar_rates.append(bipolar)\n",
    "                schizo_rates.append(schizo)\n",
    "                eating_disorder_rates.append(eating)\n",
    "    \n",
    "    # Create initial dataframe\n",
    "    combined_data['country'] = countries\n",
    "    combined_data['anxiety_rate'] = anxiety_rates\n",
    "    combined_data['bipolar_rate'] = bipolar_rates\n",
    "    combined_data['schizophrenia_rate'] = schizo_rates\n",
    "    combined_data['eating_disorder_rate'] = eating_disorder_rates\n",
    "    \n",
    "    # Add coping strategy data\n",
    "    if hasattr(self, 'dealing_anxiety_df'):\n",
    "        for idx, row in combined_data.iterrows():\n",
    "            country = row['country']\n",
    "            country_coping = self.dealing_anxiety_df[self.dealing_anxiety_df['Entity'] == country]\n",
    "            \n",
    "            if not country_coping.empty:\n",
    "                for strategy in self.coping_strategies['social'] + self.coping_strategies['lifestyle'] + \\\n",
    "                               self.coping_strategies['professional'] + self.coping_strategies['spiritual']:\n",
    "                    if strategy in country_coping.columns:\n",
    "                        combined_data.loc[idx, f\"strategy_{strategy}\"] = country_coping[strategy].values[0]\n",
    "    \n",
    "    # Create symptom-condition mapping data\n",
    "    symptom_data = []\n",
    "    \n",
    "    # For each condition and its symptoms, create examples\n",
    "    for condition, data in self.conditions.items():\n",
    "        for symptom in data['symptoms']:\n",
    "            # Create multiple examples with various symptom combinations\n",
    "            for i in range(5):  # Create 5 examples per symptom\n",
    "                # Select 1-3 additional random symptoms for this condition\n",
    "                additional_symptoms = random.sample([s for s in data['symptoms'] if s != symptom], \n",
    "                                                 random.randint(1, min(3, len(data['symptoms'])-1)))\n",
    "                all_symptoms = [symptom] + additional_symptoms\n",
    "                \n",
    "                # Create a record\n",
    "                record = {'symptoms': all_symptoms, 'condition': condition}\n",
    "                \n",
    "                # Add random duration\n",
    "                record['duration_days'] = random.choice([7, 14, 30, 90, 180, 365])\n",
    "                \n",
    "                # Add random country\n",
    "                record['country'] = random.choice(countries)\n",
    "                \n",
    "                # Add to dataset\n",
    "                symptom_data.append(record)\n",
    "    \n",
    "    symptom_df = pd.DataFrame(symptom_data)\n",
    "    \n",
    "    # Save the datasets\n",
    "    combined_data.to_csv('ml_training_country_data.csv', index=False)\n",
    "    symptom_df.to_csv('ml_training_symptom_data.csv', index=False)\n",
    "    \n",
    "    return combined_data, symptom_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccdd931f-91d7-42cf-8b71-5231bb11972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(self, data):\n",
    "    \"\"\"\n",
    "    Create new features from existing data\n",
    "    \n",
    "    Parameters:\n",
    "    data (DataFrame): Raw data with symptoms, country, etc.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Engineered features\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=data.index)\n",
    "    \n",
    "    # 1. Symptom count - how many symptoms reported\n",
    "    if 'symptoms' in data.columns:\n",
    "        features['symptom_count'] = data['symptoms'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    \n",
    "    # 2. Symptom severity score - weighted by importance\n",
    "    # Define symptom weights (anxiety symptoms might be more predictive of anxiety, etc.)\n",
    "    symptom_weights = {\n",
    "        'panic attacks': 3,\n",
    "        'hallucinations': 3,\n",
    "        'delusions': 3,\n",
    "        'mood swings': 2.5,\n",
    "        'excessive worry': 2,\n",
    "        'restlessness': 1.5,\n",
    "        'fatigue': 1,\n",
    "        'sleep problems': 1.5,\n",
    "        # Add more symptoms with weights\n",
    "    }\n",
    "    \n",
    "    if 'symptoms' in data.columns:\n",
    "        features['symptom_severity'] = data['symptoms'].apply(\n",
    "            lambda x: sum(symptom_weights.get(s, 1) for s in x) if isinstance(x, list) else 0\n",
    "        )\n",
    "    \n",
    "    # 3. Duration factors (longer duration suggests chronic issues)\n",
    "    if 'duration_days' in data.columns:\n",
    "        features['short_term'] = data['duration_days'].apply(lambda x: 1 if x < 30 else 0)\n",
    "        features['medium_term'] = data['duration_days'].apply(lambda x: 1 if 30 <= x < 180 else 0)\n",
    "        features['long_term'] = data['duration_days'].apply(lambda x: 1 if x >= 180 else 0)\n",
    "        \n",
    "        # Duration severity score (log transform to reduce skew)\n",
    "        features['duration_severity'] = data['duration_days'].apply(lambda x: np.log1p(x))\n",
    "    \n",
    "    # 4. Country-specific prevalence features\n",
    "    if 'country' in data.columns and hasattr(self, 'disorders_df_latest'):\n",
    "        # Create empty columns first\n",
    "        features['country_anxiety_rate'] = np.nan\n",
    "        features['country_bipolar_rate'] = np.nan\n",
    "        features['country_schizo_rate'] = np.nan\n",
    "        features['country_eating_rate'] = np.nan\n",
    "        \n",
    "        for idx, country in enumerate(data['country']):\n",
    "            country_data = self.disorders_df_latest[self.disorders_df_latest['Entity'] == country]\n",
    "            if not country_data.empty:\n",
    "                # Fill in rates for this country\n",
    "                if 'Anxiety' in country_data.columns:\n",
    "                    features.loc[idx, 'country_anxiety_rate'] = country_data['Anxiety'].values[0]\n",
    "                if 'Bipolar' in country_data.columns:\n",
    "                    features.loc[idx, 'country_bipolar_rate'] = country_data['Bipolar'].values[0]\n",
    "                if 'Schizophrenia' in country_data.columns:\n",
    "                    features.loc[idx, 'country_schizo_rate'] = country_data['Schizophrenia'].values[0]\n",
    "                if 'Eating Disorders' in country_data.columns:\n",
    "                    features.loc[idx, 'country_eating_rate'] = country_data['Eating Disorders'].values[0]\n",
    "    \n",
    "    # 5. Interaction terms\n",
    "    if 'symptom_count' in features.columns and 'duration_severity' in features.columns:\n",
    "        features['symptom_duration_interaction'] = features['symptom_count'] * features['duration_severity']\n",
    "    \n",
    "    # 6. One-hot encode the country\n",
    "    if 'country' in data.columns:\n",
    "        country_dummies = pd.get_dummies(data['country'], prefix='country')\n",
    "        features = pd.concat([features, country_dummies], axis=1)\n",
    "    \n",
    "    # Fill any remaining NAs with 0\n",
    "    features = features.fillna(0)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63671282-3537-41b9-a196-9f0b380709f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(self, features, target, method='forest'):\n",
    "    \"\"\"\n",
    "    Select most important features using different methods\n",
    "    \n",
    "    Parameters:\n",
    "    features (DataFrame): Engineered features\n",
    "    target (Series): Target variable to predict\n",
    "    method (str): Method to use ('forest', 'correlation', or 'chi2')\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (selected_features DataFrame, feature_indices list)\n",
    "    \"\"\"\n",
    "    print(f\"Performing feature selection using {method} method...\")\n",
    "    \n",
    "    # For classification targets, ensure they're encoded\n",
    "    if target.dtype == 'object':\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        encoded_target = label_encoder.fit_transform(target)\n",
    "    else:\n",
    "        encoded_target = target\n",
    "    \n",
    "    if method == 'forest':\n",
    "        # Random Forest for feature importance\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        # Handle case where we have very few samples\n",
    "        n_estimators = min(100, features.shape[0])\n",
    "        \n",
    "        rf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
    "        rf.fit(features, encoded_target)\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = rf.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        # Display feature importances\n",
    "        print(\"\\nFeature ranking:\")\n",
    "        for i, idx in enumerate(indices[:10]):  # Show top 10\n",
    "            print(f\"{i+1}. Feature '{features.columns[idx]}' - importance: {importances[idx]:.4f}\")\n",
    "        \n",
    "        # Select top 70% most important features\n",
    "        cumulative_importance = 0.0\n",
    "        feature_indices = []\n",
    "        for idx in indices:\n",
    "            cumulative_importance += importances[idx]\n",
    "            feature_indices.append(idx)\n",
    "            if cumulative_importance >= 0.7:\n",
    "                break\n",
    "        \n",
    "    elif method == 'correlation':\n",
    "        # Correlation analysis\n",
    "        import scipy.stats as stats\n",
    "        \n",
    "        correlations = []\n",
    "        for i, col in enumerate(features.columns):\n",
    "            if features[col].dtype in ['int64', 'float64']:\n",
    "                corr, _ = stats.pointbiserialr(features[col], encoded_target)\n",
    "                correlations.append((i, abs(corr)))\n",
    "        \n",
    "        # Sort by correlation strength (descending)\n",
    "        correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Select top 70% features\n",
    "        top_70_percent = int(len(correlations) * 0.7)\n",
    "        feature_indices = [idx for idx, _ in correlations[:top_70_percent]]\n",
    "        \n",
    "    elif method == 'chi2':\n",
    "        # Chi-square test (for categorical features)\n",
    "        from sklearn.feature_selection import chi2, SelectKBest\n",
    "        \n",
    "        # Determine number of features to select (70%)\n",
    "        k = int(features.shape[1] * 0.7)\n",
    "        \n",
    "        # Apply chi-square test\n",
    "        selector = SelectKBest(chi2, k=k)\n",
    "        selector.fit(features, encoded_target)\n",
    "        feature_indices = selector.get_support(indices=True)\n",
    "    \n",
    "    # Get the selected features\n",
    "    selected_features = features.iloc[:, feature_indices]\n",
    "    \n",
    "    print(f\"Selected {selected_features.shape[1]} features out of {features.shape[1]}\")\n",
    "    return selected_features, feature_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ca5ff7-b526-45ce-a1cd-b1bc46f4aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(self, features, condition_labels, strategy_data=None):\n",
    "    \"\"\"\n",
    "    Train the required ML models for the chatbot\n",
    "    \n",
    "    Parameters:\n",
    "    features (DataFrame): Engineered and selected features\n",
    "    condition_labels (Series): Condition labels for classification\n",
    "    strategy_data (DataFrame): Data for strategy recommendation model\n",
    "    \n",
    "    Returns:\n",
    "    dict: Performance metrics for the models\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "    \n",
    "    print(\"Training machine learning models...\")\n",
    "    \n",
    "    # Standardize numerical features\n",
    "    self.scaler = StandardScaler()\n",
    "    numerical_cols = features.select_dtypes(include=['int64', 'float64']).columns\n",
    "    features[numerical_cols] = self.scaler.fit_transform(features[numerical_cols])\n",
    "    \n",
    "    # Encode condition labels\n",
    "    self.label_encoder = LabelEncoder()\n",
    "    y_encoded = self.label_encoder.fit_transform(condition_labels)\n",
    "    self.classes_ = self.label_encoder.classes_\n",
    "    \n",
    "    # Split the data (70% train, 15% validation, 15% test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        features, y_encoded, test_size=0.3, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"Training set size: {X_train.shape[0]}\")\n",
    "    print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "    print(f\"Test set size: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Model 1: Condition classifier (Random Forest)\n",
    "    print(\"\\nTraining Random Forest Classifier for condition prediction...\")\n",
    "    self.rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    self.rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_rf = self.rf_model.predict(X_val)\n",
    "    rf_accuracy = accuracy_score(y_val, y_pred_rf)\n",
    "    rf_precision = precision_score(y_val, y_pred_rf, average='weighted')\n",
    "    rf_recall = recall_score(y_val, y_pred_rf, average='weighted')\n",
    "    rf_f1 = f1_score(y_val, y_pred_rf, average='weighted')\n",
    "    \n",
    "    print(f\"Random Forest Validation Results:\")\n",
    "    print(f\"  Accuracy: {rf_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {rf_precision:.4f}\")\n",
    "    print(f\"  Recall: {rf_recall:.4f}\")\n",
    "    print(f\"  F1 Score: {rf_f1:.4f}\")\n",
    "    \n",
    "    # Model 2: Condition classifier (Logistic Regression)\n",
    "    print(\"\\nTraining Logistic Regression Classifier for condition prediction...\")\n",
    "    self.lr_model = OneVsRestClassifier(LogisticRegression(max_iter=1000, random_state=42))\n",
    "    self.lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_lr = self.lr_model.predict(X_val)\n",
    "    lr_accuracy = accuracy_score(y_val, y_pred_lr)\n",
    "    lr_precision = precision_score(y_val, y_pred_lr, average='weighted')\n",
    "    lr_recall = recall_score(y_val, y_pred_lr, average='weighted')\n",
    "    lr_f1 = f1_score(y_val, y_pred_lr, average='weighted')\n",
    "    \n",
    "    print(f\"Logistic Regression Validation Results:\")\n",
    "    print(f\"  Accuracy: {lr_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {lr_precision:.4f}\")\n",
    "    print(f\"  Recall: {lr_recall:.4f}\")\n",
    "    print(f\"  F1 Score: {lr_f1:.4f}\")\n",
    "    \n",
    "    # Model.3: Condition classifier (SVM)\n",
    "    print(\"\\nTraining SVM Classifier for condition prediction...\")\n",
    "    self.svm_model = SVC(probability=True, random_state=42)\n",
    "    self.svm_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_svm = self.svm_model.predict(X_val)\n",
    "    svm_accuracy = accuracy_score(y_val, y_pred_svm)\n",
    "    svm_precision = precision_score(y_val, y_pred_svm, average='weighted')\n",
    "    svm_recall = recall_score(y_val, y_pred_svm, average='weighted')\n",
    "    svm_f1 = f1_score(y_val, y_pred_svm, average='weighted')\n",
    "    \n",
    "    print(f\"SVM Validation Results:\")\n",
    "    print(f\"  Accuracy: {svm_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {svm_precision:.4f}\")\n",
    "    print(f\"  Recall: {svm_recall:.4f}\")\n",
    "    print(f\"  F1 Score: {svm_f1:.4f}\")\n",
    "    \n",
    "    # Compare models and select the best one\n",
    "    models = {\n",
    "        'Random Forest': (self.rf_model, rf_accuracy, rf_precision, rf_recall, rf_f1),\n",
    "        'Logistic Regression': (self.lr_model, lr_accuracy, lr_precision, lr_recall, lr_f1),\n",
    "        'SVM': (self.svm_model, svm_accuracy, svm_precision, svm_recall, svm_f1)\n",
    "    }\n",
    "    \n",
    "    best_model_name = max(models.items(), key=lambda x: x[1][1])[0]  # Based on accuracy\n",
    "    self.best_model, best_accuracy, best_precision, best_recall, best_f1 = models[best_model_name]\n",
    "    \n",
    "    print(f\"\\nBest model: {best_model_name} with accuracy {best_accuracy:.4f}\")\n",
    "    \n",
    "    # Final evaluation on test set\n",
    "    y_pred_best = self.best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_best)\n",
    "    test_precision = precision_score(y_test, y_pred_best, average='weighted')\n",
    "    test_recall = recall_score(y_test, y_pred_best, average='weighted')\n",
    "    test_f1 = f1_score(y_test, y_pred_best, average='weighted')\n",
    "    \n",
    "    print(f\"\\nBest Model Test Results:\")\n",
    "    print(f\"  Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {test_precision:.4f}\")\n",
    "    print(f\"  Recall: {test_recall:.4f}\")\n",
    "    print(f\"  F1 Score: {test_f1:.4f}\")\n",
    "    \n",
    "    # Return performance metrics\n",
    "    return {\n",
    "        'train_size': X_train.shape[0],\n",
    "        'val_size': X_val.shape[0],\n",
    "        'test_size': X_test.shape[0],\n",
    "        'models': {\n",
    "            'random_forest': {\n",
    "                'accuracy': rf_accuracy,\n",
    "                'precision': rf_precision,\n",
    "                'recall': rf_recall,\n",
    "                'f1': rf_f1\n",
    "            },\n",
    "            'logistic_regression': {\n",
    "                'accuracy': lr_accuracy,\n",
    "                'precision': lr_precision,\n",
    "                'recall': lr_recall,\n",
    "                'f1': lr_f1\n",
    "            },\n",
    "            'svm': {\n",
    "                'accuracy': svm_accuracy,\n",
    "                'precision': svm_precision,\n",
    "                'recall': svm_recall,\n",
    "                'f1': svm_f1\n",
    "            }\n",
    "        },\n",
    "        'best_model': {\n",
    "            'name': best_model_name,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'test_precision': test_precision,\n",
    "            'test_recall': test_recall,\n",
    "            'test_f1': test_f1\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba8537f-8c73-4184-a8bc-f37fee6a4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_condition(self, symptoms, duration_days=30, country=\"Global\"):\n",
    "    \"\"\"\n",
    "    Predict the most likely mental health condition based on symptoms\n",
    "    \n",
    "    Parameters:\n",
    "    symptoms (list): List of symptom strings\n",
    "    duration_days (int): Duration of symptoms in days\n",
    "    country (str): User's country\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (predicted_condition, confidence_score)\n",
    "    \"\"\"\n",
    "    if not hasattr(self, 'best_model') or self.best_model is None:\n",
    "        # If models aren't trained, return None\n",
    "        return None, 0\n",
    "    \n",
    "    # Create a sample for prediction\n",
    "    sample = pd.DataFrame({\n",
    "        'symptoms': [symptoms],\n",
    "        'duration_days': [duration_days],\n",
    "        'country': [country]\n",
    "    })\n",
    "    \n",
    "    # Engineer features\n",
    "    features = self.engineer_features(sample)\n",
    "    \n",
    "    # Apply feature selection if available\n",
    "    if hasattr(self, 'selected_features') and self.selected_features is not None:\n",
    "        features = features[self.selected_features]\n",
    "    \n",
    "    # Scale numerical features\n",
    "    if hasattr(self, 'scaler') and self.scaler is not None:\n",
    "        numerical_cols = features.select_dtypes(include=['int64', 'float64']).columns\n",
    "        features[numerical_cols] = self.scaler.transform(features[numerical_cols])\n",
    "    \n",
    "    # Make prediction\n",
    "    pred_proba = self.best_model.predict_proba(features)\n",
    "    pred_class_index = np.argmax(pred_proba[0])\n",
    "    confidence = pred_proba[0][pred_class_index]\n",
    "    \n",
    "    # Convert back to condition name\n",
    "    if hasattr(self, 'label_encoder') and self.label_encoder is not None:\n",
    "        condition = self.label_encoder.inverse_transform([pred_class_index])[0]\n",
    "    else:\n",
    "        condition = self.conditions.keys()[pred_class_index]\n",
    "    \n",
    "    return condition, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb501a53-f59d-48d2-ae1b-5043185c3f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_performance(self, metrics):\n",
    "    \"\"\"\n",
    "    Generate visualizations of model performance for the report\n",
    "    \n",
    "    Parameters:\n",
    "    metrics (dict): Dictionary containing model performance metrics\n",
    "    \n",
    "    Returns:\n",
    "    list: Filenames of generated visualization images\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import os\n",
    "    \n",
    "    # Create directory for visualizations if it doesn't exist\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    \n",
    "    # Set style for all plots\n",
    "    plt.style.use('ggplot')\n",
    "    generated_files = []\n",
    "    \n",
    "    # 1. Bar chart comparing accuracy across models\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    models = list(metrics['models'].keys())\n",
    "    model_names = [model.replace('_', ' ').title() for model in models]\n",
    "    accuracy_values = [metrics['models'][m]['accuracy'] for m in models]\n",
    "    \n",
    "    bars = plt.bar(model_names, accuracy_values, color=['#3498db', '#2ecc71', '#e74c3c'])\n",
    "    plt.title('Model Accuracy Comparison', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Model', fontsize=14)\n",
    "    plt.ylabel('Accuracy', fontsize=14)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{height:.3f}', ha='center', fontsize=12)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    filename = 'visualizations/model_accuracy_comparison.png'\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.close()\n",
    "    generated_files.append(filename)\n",
    "    \n",
    "    # 2. Multi-metric comparison chart\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.2\n",
    "    \n",
    "    # Extract all metrics\n",
    "    accuracies = [metrics['models'][m]['accuracy'] for m in models]\n",
    "    precisions = [metrics['models'][m]['precision'] for m in models]\n",
    "    recalls = [metrics['models'][m]['recall'] for m in models]\n",
    "    f1_scores = [metrics['models'][m]['f1'] for m in models]\n",
    "    \n",
    "    # Plot bars\n",
    "    plt.bar(x - width*1.5, accuracies, width, label='Accuracy', color='#3498db')\n",
    "    plt.bar(x - width/2, precisions, width, label='Precision', color='#2ecc71')\n",
    "    plt.bar(x + width/2, recalls, width, label='Recall', color='#e74c3c')\n",
    "    plt.bar(x + width*1.5, f1_scores, width, label='F1 Score', color='#9b59b6')\n",
    "    \n",
    "    plt.title('Model Performance Metrics Comparison', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(x, model_names, fontsize=12)\n",
    "    plt.xlabel('Model', fontsize=14)\n",
    "    plt.ylabel('Score', fontsize=14)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = 'visualizations/model_metrics_comparison.png'\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.close()\n",
    "    generated_files.append(filename)\n",
    "    \n",
    "    # 3. Confusion matrix for the best model (if available)\n",
    "    if 'confusion_matrix' in metrics:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        cm = metrics['confusion_matrix']\n",
    "        \n",
    "        # Get class names\n",
    "        if 'classes' in metrics:\n",
    "            class_names = metrics['classes']\n",
    "        else:\n",
    "            class_names = [f\"Class {i}\" for i in range(len(cm))]\n",
    "        \n",
    "        # Plot the confusion matrix\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f\"Confusion Matrix - {metrics['best_model']['name']}\", fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Predicted Label', fontsize=14)\n",
    "        plt.ylabel('True Label', fontsize=14)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = 'visualizations/confusion_matrix.png'\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        plt.close()\n",
    "        generated_files.append(filename)\n",
    "    \n",
    "    # 4. ROC curves for each model (if available)\n",
    "    if all(key in metrics for key in ['fpr', 'tpr', 'roc_auc']):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for i, model_name in enumerate(model_names):\n",
    "            if i < len(metrics['fpr']):\n",
    "                fpr = metrics['fpr'][i]\n",
    "                tpr = metrics['tpr'][i]\n",
    "                roc_auc = metrics['roc_auc'][i]\n",
    "                \n",
    "                plt.plot(fpr, tpr, lw=2, \n",
    "                         label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate', fontsize=14)\n",
    "        plt.ylabel('True Positive Rate', fontsize=14)\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curves', fontsize=16, fontweight='bold')\n",
    "        plt.legend(loc=\"lower right\", fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = 'visualizations/roc_curves.png'\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        plt.close()\n",
    "        generated_files.append(filename)\n",
    "    \n",
    "    # 5. Learning curves for the best model (if available)\n",
    "    if 'learning_curve' in metrics:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        train_sizes = metrics['learning_curve']['train_sizes']\n",
    "        train_scores = metrics['learning_curve']['train_scores']\n",
    "        val_scores = metrics['learning_curve']['val_scores']\n",
    "        \n",
    "        plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', color='#3498db', \n",
    "                 label='Training score')\n",
    "        plt.plot(train_sizes, np.mean(val_scores, axis=1), 'o-', color='#e74c3c', \n",
    "                 label='Validation score')\n",
    "        \n",
    "        plt.title(f'Learning Curves - {metrics[\"best_model\"][\"name\"]}', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Training Examples', fontsize=14)\n",
    "        plt.ylabel('Score', fontsize=14)\n",
    "        plt.legend(loc='best', fontsize=12)\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = 'visualizations/learning_curves.png'\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        plt.close()\n",
    "        generated_files.append(filename)\n",
    "    \n",
    "    return generated_files\n",
    "\n",
    "def visualize_feature_importance(self, model=None, feature_names=None):\n",
    "    \"\"\"\n",
    "    Generate feature importance visualization\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained model with feature_importances_ attribute (optional)\n",
    "    feature_names (list): List of feature names (optional)\n",
    "    \n",
    "    Returns:\n",
    "    list: Filenames of generated visualization images\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import os\n",
    "    \n",
    "    # Create directory for visualizations if it doesn't exist\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    generated_files = []\n",
    "    \n",
    "    # Use provided model or try to find one\n",
    "    if model is None:\n",
    "        if hasattr(self, 'rf_model'):\n",
    "            model = self.rf_model\n",
    "        elif hasattr(self, 'best_model'):\n",
    "            model = self.best_model\n",
    "        else:\n",
    "            print(\"No model available for feature importance visualization\")\n",
    "            return generated_files\n",
    "    \n",
    "    # Make sure model has feature_importances_\n",
    "    if not hasattr(model, 'feature_importances_'):\n",
    "        # Check if it's a pipeline or ensemble\n",
    "        if hasattr(model, 'steps'):\n",
    "            for name, step in model.steps:\n",
    "                if hasattr(step, 'feature_importances_'):\n",
    "                    model = step\n",
    "                    break\n",
    "        elif hasattr(model, 'estimators_'):\n",
    "            # For ensemble models, try to access the first estimator\n",
    "            if len(model.estimators_) > 0:\n",
    "                if hasattr(model.estimators_[0], 'feature_importances_'):\n",
    "                    model = model.estimators_[0]\n",
    "    \n",
    "    # If we still don't have feature_importances_, exit\n",
    "    if not hasattr(model, 'feature_importances_'):\n",
    "        print(\"No feature importance information available in the model\")\n",
    "        return generated_files\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    # Use provided feature names or try to get them from the model\n",
    "    if feature_names is None:\n",
    "        if hasattr(self, 'selected_features'):\n",
    "            feature_names = self.selected_features\n",
    "        elif hasattr(model, 'feature_names_in_'):\n",
    "            feature_names = model.feature_names_in_\n",
    "        else:\n",
    "            feature_names = [f\"Feature {i}\" for i in range(len(importances))]\n",
    "    \n",
    "    # Make sure feature_names is the right length\n",
    "    if len(feature_names) != len(importances):\n",
    "        feature_names = [f\"Feature {i}\" for i in range(len(importances))]\n",
    "    \n",
    "    # Sort features by importance\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # 1. Bar chart of top features\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot top 15 features (or all if fewer than 15)\n",
    "    num_features = min(15, len(indices))\n",
    "    \n",
    "    # Get color gradient\n",
    "    colors = plt.cm.viridis(np.linspace(0, 0.8, num_features))\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    plt.barh(range(num_features), importances[indices[:num_features]], align='center', color=colors)\n",
    "    plt.yticks(range(num_features), [feature_names[i] for i in indices[:num_features]])\n",
    "    plt.xlabel('Feature Importance', fontsize=14)\n",
    "    plt.title('Top Feature Importances', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Add value labels to bars\n",
    "    for i, v in enumerate(importances[indices[:num_features]]):\n",
    "        plt.text(v + 0.01, i, f\"{v:.3f}\", va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = 'visualizations/feature_importance_bar.png'\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.close()\n",
    "    generated_files.append(filename)\n",
    "    \n",
    "    # 2. Pie chart of top features' importance distribution\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Select top 10 features for pie chart\n",
    "    num_features_pie = min(10, len(indices))\n",
    "    \n",
    "    # Get values and labels\n",
    "    values = importances[indices[:num_features_pie]]\n",
    "    labels = [feature_names[i] for i in indices[:num_features_pie]]\n",
    "    \n",
    "    # If there are more features, add an \"Other\" category\n",
    "    if len(indices) > num_features_pie:\n",
    "        values = np.append(values, np.sum(importances[indices[num_features_pie:]]))\n",
    "        labels.append('Other Features')\n",
    "    \n",
    "    # Create pie chart\n",
    "    plt.pie(values, labels=None, autopct='%1.1f%%', startangle=90, \n",
    "            shadow=True, colors=plt.cm.tab10.colors[:len(values)])\n",
    "    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "    plt.title('Distribution of Feature Importance (Top 10)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Add legend with percentages\n",
    "    plt.legend(labels, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = 'visualizations/feature_importance_pie.png'\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.close()\n",
    "    generated_files.append(filename)\n",
    "    \n",
    "    # 3. Feature importance grouped by category (if categories are known)\n",
    "    if hasattr(self, 'feature_categories') and isinstance(self.feature_categories, dict):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Group features by category\n",
    "        category_importance = {}\n",
    "        for i, feature in enumerate(feature_names):\n",
    "            # Find category for this feature\n",
    "            for category, features in self.feature_categories.items():\n",
    "                if any(feature.startswith(f) or feature == f for f in features):\n",
    "                    if category not in category_importance:\n",
    "                        category_importance[category] = 0\n",
    "                    category_importance[category] += importances[i]\n",
    "                    break\n",
    "        \n",
    "        # Sort categories by importance\n",
    "        sorted_categories = sorted(category_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        categories = [item[0] for item in sorted_categories]\n",
    "        values = [item[1] for item in sorted_categories]\n",
    "        \n",
    "        # Plot grouped importance\n",
    "        plt.bar(categories, values, color=plt.cm.Set3.colors[:len(categories)])\n",
    "        plt.title('Feature Importance by Category', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Category', fontsize=14)\n",
    "        plt.ylabel('Total Importance', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(values):\n",
    "            plt.text(i, v + 0.01, f\"{v:.3f}\", ha='center', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = 'visualizations/feature_importance_by_category.png'\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        plt.close()\n",
    "        generated_files.append(filename)\n",
    "    \n",
    "    return generated_files\n",
    "\n",
    "def visualize_data_distributions(self, data):\n",
    "    \"\"\"\n",
    "    Generate visualizations of data distributions for the report\n",
    "    \n",
    "    Parameters:\n",
    "    data (DataFrame): The dataset to visualize\n",
    "    \n",
    "    Returns:\n",
    "    list: Filenames of generated visualization images\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import os\n",
    "    \n",
    "    # Create directory for visualizations if it doesn't exist\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    generated_files = []\n",
    "    \n",
    "    # 1. Condition distribution (target variable)\n",
    "    if 'condition' in data.columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Count conditions\n",
    "        condition_counts = data['condition'].value_counts()\n",
    "        \n",
    "        # Create bar chart\n",
    "        colors = plt.cm.Paired(np.linspace(0, 1, len(condition_counts)))\n",
    "        bars = plt.bar(condition_counts.index, condition_counts.values, color=colors)\n",
    "        \n",
    "        plt.title('Distribution of Mental Health Conditions in Dataset', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Condition', fontsize=14)\n",
    "        plt.ylabel('Count', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                    f'{height}', ha='center', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = 'visualizations/condition_distribution.png'\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        plt.close()\n",
    "        generated_files.append(filename)\n",
    "    \n",
    "    # 2. Symptom frequency\n",
    "    if 'symptoms' in data.columns:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Count frequency of each symptom\n",
    "        symptom_counts = {}\n",
    "        for symptom_list in data['symptoms']:\n",
    "            if isinstance(symptom_list, list):\n",
    "                for symptom in symptom_list:\n",
    "                    symptom_counts[symptom] = symptom_counts.get(symptom, 0) + 1\n",
    "        \n",
    "        # Sort by frequency\n",
    "        symptom_counts = dict(sorted(symptom_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "        \n",
    "        # Plot top 15 symptoms\n",
    "        top_symptoms = list(symptom_counts.keys())[:15]\n",
    "        top_counts = [symptom_counts[s] for s in top_symptoms]\n",
    "        \n",
    "        # Create horizontal bar chart\n",
    "        bars = plt.barh(top_symptoms, top_counts, color=plt.cm.viridis(np.linspace(0, 0.8, len(top_symptoms))))\n",
    "        \n",
    "        plt.title('Top 15 Most Frequent Symptoms', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Frequency', fontsize=14)\n",
    "        plt.ylabel('Symptom', fontsize=14)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(top_counts):\n",
    "            plt.text(v + 0.5, i, str(v), va='center', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = 'visualizations/symptom_frequency.png'\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        plt.close()\n",
    "        generated_files.append(filename)\n",
    "    \n",
    "    # 3. Correlation matrix of numerical features\n",
    "    numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if len(numerical_cols) > 1:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Compute correlation matrix\n",
    "        corr_matrix = data[numerical_cols].corr()\n",
    "        \n",
    "        # Plot heatmap\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', annot=True, fmt=\".2f\", \n",
    "                   square=True, linewidths=.5, cbar_kws={\"shrink\": .8})\n",
    "        \n",
    "        plt.title('Correlation Matrix of Numerical Features', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        filename = 'visualizations/correlation_matrix.png'\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        plt.close()\n",
    "        generated_files.append(filename)\n",
    "    \n",
    "    # 4. Symptom co-occurrence (if we have symptom data)\n",
    "    if 'symptoms' in data.columns:\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        \n",
    "        # Identify top 10 most common symptoms\n",
    "        symptom_counts = {}\n",
    "        for symptom_list in data['symptoms']:\n",
    "            if isinstance(symptom_list, list):\n",
    "                for symptom in symptom_list:\n",
    "                    symptom_counts[symptom] = symptom_counts.get(symptom, 0) + 1\n",
    "        \n",
    "        top_symptoms = [s for s, _ in sorted(symptom_counts.items(), key=lambda item: item[1], reverse=True)[:10]]\n",
    "        \n",
    "        # Create co-occurrence matrix\n",
    "        co_occurrence = np.zeros((len(top_symptoms), len(top_symptoms)))\n",
    "        \n",
    "        for symptom_list in data['symptoms']:\n",
    "            if isinstance(symptom_list, list):\n",
    "                for i, s1 in enumerate(top_symptoms):\n",
    "                    for j, s2 in enumerate(top_symptoms):\n",
    "                        if s1 in symptom_list and s2 in symptom_list:\n",
    "                            co_occurrence[i][j] += 1\n",
    "        \n",
    "        # Convert to correlation\n",
    "        for i in range(len(top_symptoms)):\n",
    "            co_occurrence[i][i] = symptom_counts[top_symptoms[i]]\n",
    "            \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(co_occurrence, annot=True, fmt=\"d\", cmap='YlGnBu',\n",
    "                   xticklabels=top_symptoms, yticklabels=top_symptoms)\n",
    "        \n",
    "        plt.title('Symptom Co-occurrence Matrix (Top 10 Symptoms)', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        filename = 'visualizations/symptom_co_occurrence.png'\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        plt.close()\n",
    "        generated_files.append(filename)\n",
    "    \n",
    "    # 5. Condition by country (if country data is available)\n",
    "    if 'country' in data.columns and 'condition' in data.columns:\n",
    "        # Get top 5 countries by frequency\n",
    "        top_countries = data['country'].value_counts().nlargest(5).index.tolist()\n",
    "        \n",
    "        # Filter data for top countries\n",
    "        top_countries_data = data[data['country'].isin(top_countries)]\n",
    "        \n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Create count plot\n",
    "        ax = sns.countplot(x='country', hue='condition', data=top_countries_data, palette='tab10')\n",
    "        \n",
    "        plt.title('Conditions by Country (Top 5 Countries)', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Country', fontsize=14)\n",
    "        plt.ylabel('Count', fontsize=14)\n",
    "        plt.legend(title='Condition', fontsize=12)\n",
    "        \n",
    "        # Rotate x-axis labels\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Add count labels\n",
    "        for p in ax.patches:\n",
    "            height = p.get_height()\n",
    "            if height > 0:\n",
    "                ax.text(p.get_x() + p.get_width()/2., height + 0.5,\n",
    "                        f'{int(height)}', ha='center', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = 'visualizations/condition_by_country.png'\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        plt.close()\n",
    "        generated_files.append(filename)\n",
    "    \n",
    "    return generated_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c9b8b78-d837-4fa3-b4da-0ae4f205af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ml_pipeline(self):\n",
    "    \"\"\"Run the complete ML pipeline for the project\"\"\"\n",
    "    print(\"Starting Mental Health Assistant ML Pipeline...\")\n",
    "    \n",
    "    # 1. Prepare training data\n",
    "    country_data, symptom_data = self.prepare_training_data()\n",
    "    print(f\"Prepared training data: {symptom_data.shape[0]} symptom records\")\n",
    "    \n",
    "    # 2. Engineer features\n",
    "    features = self.engineer_features(symptom_data)\n",
    "    print(f\"Engineered {features.shape[1]} features\")\n",
    "    \n",
    "    # 3. Select features\n",
    "    selected_features, feature_indices = self.select_features(features, symptom_data['condition'])\n",
    "    self.selected_features = selected_features.columns.tolist()\n",
    "    print(f\"Selected {len(self.selected_features)} features\")\n",
    "    \n",
    "    # 4. Train models\n",
    "    metrics = self.train_models(selected_features, symptom_data['condition'])\n",
    "    print(\"Model training completed\")\n",
    "    \n",
    "    # 5. Visualize results\n",
    "    performance_charts = self.visualize_model_performance(metrics)\n",
    "    importance_charts = self.visualize_feature_importance()\n",
    "    \n",
    "    print(\"ML pipeline completed successfully!\")\n",
    "    print(f\"Generated visualization files: {performance_charts + importance_charts}\")\n",
    "    \n",
    "    # Return summary metrics\n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'selected_features': self.selected_features,\n",
    "        'visualizations': performance_charts + importance_charts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d128a245-22b6-477d-904b-ea7588e05fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MentalHealthAssistant class is not defined in this notebook\n"
     ]
    }
   ],
   "source": [
    "# Check if MentalHealthAssistant is already defined\n",
    "if 'MentalHealthAssistant' in globals():\n",
    "    print(\"MentalHealthAssistant class is already defined\")\n",
    "else:\n",
    "    print(\"MentalHealthAssistant class is not defined in this notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b17e1d5-e507-4bf5-9485-b6b41fc4caf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting mental health ML pipeline...\n",
      "Initializing simplified Mental Health Assistant for ML pipeline...\n",
      "Warning: No disorders data available. Using synthetic data for demonstration.\n",
      "\n",
      "Step 1: Preprocessing data...\n",
      "\n",
      "Step 2: Training and evaluating models...\n",
      "\n",
      "Processing anxiety...\n",
      "\n",
      "Processing depression...\n",
      "\n",
      "Processing bipolar...\n",
      "\n",
      "Processing schizophrenia...\n",
      "\n",
      "Processing eating disorders...\n",
      "\n",
      "Step 3: Generating insights...\n",
      "\n",
      "Summary for Milestone 2 Report:\n",
      "========================================\n",
      "\n",
      "Model Performance:\n",
      "- Anxiety: Best model = logistic_regression, F1 Score = 0.3750\n",
      "- Depression: Best model = logistic_regression, F1 Score = 0.6667\n",
      "- Bipolar: Best model = random_forest, F1 Score = 0.6316\n",
      "- Schizophrenia: Best model = logistic_regression, F1 Score = 0.5000\n",
      "- Eating Disorders: Best model = random_forest, F1 Score = 0.4615\n",
      "\n",
      "Top Predictors by Condition:\n",
      "\n",
      "Anxiety top predictors:\n",
      "  1. GDP_per_capita (importance: 0.4183)\n",
      "  2. Year (importance: 0.4008)\n",
      "  3. country_United Kingdom (importance: 0.0436)\n",
      "  4. country_United States (importance: 0.0416)\n",
      "  5. country_Global (importance: 0.0350)\n",
      "\n",
      "Depression top predictors:\n",
      "  1. GDP_per_capita (importance: 0.3970)\n",
      "  2. Year (importance: 0.3428)\n",
      "  3. country_Canada (importance: 0.0659)\n",
      "  4. country_Global (importance: 0.0563)\n",
      "  5. country_India (importance: 0.0516)\n",
      "\n",
      "Bipolar top predictors:\n",
      "  1. GDP_per_capita (importance: 0.4873)\n",
      "  2. Year (importance: 0.3406)\n",
      "  3. country_Global (importance: 0.0439)\n",
      "  4. country_United States (importance: 0.0371)\n",
      "  5. country_Canada (importance: 0.0329)\n",
      "\n",
      "Schizophrenia top predictors:\n",
      "  1. GDP_per_capita (importance: 0.4402)\n",
      "  2. Year (importance: 0.3572)\n",
      "  3. country_Global (importance: 0.0570)\n",
      "  4. country_United Kingdom (importance: 0.0431)\n",
      "  5. country_India (importance: 0.0389)\n",
      "\n",
      "Eating Disorders top predictors:\n",
      "  1. GDP_per_capita (importance: 0.4196)\n",
      "  2. Year (importance: 0.3749)\n",
      "  3. country_India (importance: 0.0553)\n",
      "  4. country_Canada (importance: 0.0492)\n",
      "  5. country_United States (importance: 0.0478)\n",
      "\n",
      "Country Comparison (Latest Year):\n",
      "\n",
      "Anxiety prevalence:\n",
      "  - Australia: 7.45%\n",
      "  - United States: 4.63%\n",
      "  - United Kingdom: 6.67%\n",
      "  - Canada: 5.11%\n",
      "  - India: 5.24%\n",
      "  - Global: 11.09%\n",
      "\n",
      "Depression prevalence:\n",
      "  - Australia: 10.99%\n",
      "  - United States: 5.41%\n",
      "  - United Kingdom: 5.28%\n",
      "  - Canada: 9.55%\n",
      "  - India: 12.54%\n",
      "  - Global: 6.82%\n",
      "\n",
      "Bipolar prevalence:\n",
      "  - Australia: 2.44%\n",
      "  - United States: 1.25%\n",
      "  - United Kingdom: 3.51%\n",
      "  - Canada: 1.92%\n",
      "  - India: 3.21%\n",
      "  - Global: 1.02%\n",
      "\n",
      "Schizophrenia prevalence:\n",
      "  - Australia: 1.25%\n",
      "  - United States: 1.11%\n",
      "  - United Kingdom: 1.62%\n",
      "  - Canada: 0.78%\n",
      "  - India: 0.70%\n",
      "  - Global: 1.49%\n",
      "\n",
      "Eating Disorders prevalence:\n",
      "  - Australia: 3.53%\n",
      "  - United States: 5.07%\n",
      "  - United Kingdom: 3.27%\n",
      "  - Canada: 2.54%\n",
      "  - India: 5.46%\n",
      "  - Global: 2.23%\n"
     ]
    }
   ],
   "source": [
    "def run_pipeline():\n",
    "    \"\"\"\n",
    "    Run the mental health analysis pipeline:\n",
    "    1. Load and initialize the assistant\n",
    "    2. Preprocess the data\n",
    "    3. Train models for each mental health condition\n",
    "    4. Evaluate the models\n",
    "    5. Generate insights\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing results from the pipeline run\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    \n",
    "    print(\"Starting mental health ML pipeline...\")\n",
    "    \n",
    "    # Initialize the assistant\n",
    "    assistant = MentalHealthAssistant()\n",
    "    \n",
    "    # Check if data is available\n",
    "    if assistant.disorders_df is None:\n",
    "        print(\"Warning: No disorders data available. Using synthetic data for demonstration.\")\n",
    "        # Create synthetic data for demonstration\n",
    "        assistant.disorders_df = create_synthetic_data()\n",
    "        assistant.disorders_latest_year = 2023\n",
    "        assistant.disorders_df_latest = assistant.disorders_df[assistant.disorders_df['Year'] == 2023]\n",
    "    \n",
    "    # Step 1: Preprocess data\n",
    "    print(\"\\nStep 1: Preprocessing data...\")\n",
    "    X, y_dict = preprocess_data(assistant)\n",
    "    \n",
    "    # Step 2: Train and evaluate models for each condition\n",
    "    print(\"\\nStep 2: Training and evaluating models...\")\n",
    "    model_results = {}\n",
    "    \n",
    "    for condition in assistant.conditions:\n",
    "        print(f\"\\nProcessing {condition}...\")\n",
    "        column_name = assistant.conditions[condition]['column']\n",
    "        \n",
    "        # Skip if no data available for this condition\n",
    "        if column_name not in y_dict:\n",
    "            print(f\"No data available for {condition}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        y = y_dict[column_name]\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Train Random Forest\n",
    "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf_model.fit(X_train_scaled, y_train)\n",
    "        rf_preds = rf_model.predict(X_test_scaled)\n",
    "        \n",
    "        # Train Logistic Regression\n",
    "        lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "        lr_model.fit(X_train_scaled, y_train)\n",
    "        lr_preds = lr_model.predict(X_test_scaled)\n",
    "        \n",
    "        # Evaluate models\n",
    "        rf_metrics = {\n",
    "            'accuracy': accuracy_score(y_test, rf_preds),\n",
    "            'precision': precision_score(y_test, rf_preds, zero_division=0),\n",
    "            'recall': recall_score(y_test, rf_preds, zero_division=0),\n",
    "            'f1': f1_score(y_test, rf_preds, zero_division=0)\n",
    "        }\n",
    "        \n",
    "        lr_metrics = {\n",
    "            'accuracy': accuracy_score(y_test, lr_preds),\n",
    "            'precision': precision_score(y_test, lr_preds, zero_division=0),\n",
    "            'recall': recall_score(y_test, lr_preds, zero_division=0),\n",
    "            'f1': f1_score(y_test, lr_preds, zero_division=0)\n",
    "        }\n",
    "        \n",
    "        # Store results\n",
    "        model_results[condition] = {\n",
    "            'random_forest': rf_metrics,\n",
    "            'logistic_regression': lr_metrics,\n",
    "            'feature_importance': {\n",
    "                'features': X.columns.tolist(),\n",
    "                'importance': rf_model.feature_importances_.tolist()\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Step 3: Generate insights\n",
    "    print(\"\\nStep 3: Generating insights...\")\n",
    "    insights = generate_insights(assistant, model_results)\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'model_results': model_results,\n",
    "        'insights': insights\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def preprocess_data(assistant):\n",
    "    \"\"\"\n",
    "    Preprocess the data for ML modeling:\n",
    "    1. Select relevant features\n",
    "    2. Handle missing values\n",
    "    3. Encode categorical variables\n",
    "    4. Create target variables\n",
    "    \n",
    "    Args:\n",
    "        assistant: Initialized MentalHealthAssistant instance\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X, y_dict) containing features and target variables\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    df = assistant.disorders_df.copy()\n",
    "    \n",
    "    # Create feature dataframe\n",
    "    features = []\n",
    "    \n",
    "    # Add demographic features\n",
    "    features.append(pd.get_dummies(df['Entity'], prefix='country', drop_first=True))\n",
    "    \n",
    "    # Add year as a feature\n",
    "    features.append(df[['Year']])\n",
    "    \n",
    "    # Add additional computed features\n",
    "    if 'Population' in df.columns and 'GDP' in df.columns:\n",
    "        df['GDP_per_capita'] = df['GDP'] / df['Population']\n",
    "        features.append(df[['GDP_per_capita']])\n",
    "    \n",
    "    # Combine all features\n",
    "    X = pd.concat(features, axis=1)\n",
    "    \n",
    "    # Create target variables for each condition\n",
    "    y_dict = {}\n",
    "    for condition, info in assistant.conditions.items():\n",
    "        column = info['column']\n",
    "        if column in df.columns:\n",
    "            # Binarize the target - assuming values above median indicate presence of condition\n",
    "            threshold = df[column].median()\n",
    "            y_dict[column] = (df[column] > threshold).astype(int)\n",
    "    \n",
    "    return X, y_dict\n",
    "\n",
    "\n",
    "def generate_insights(assistant, model_results):\n",
    "    \"\"\"\n",
    "    Generate insights from the trained models and data\n",
    "    \n",
    "    Args:\n",
    "        assistant: Initialized MentalHealthAssistant instance\n",
    "        model_results: Dictionary containing model evaluation results\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing insights from the analysis\n",
    "    \"\"\"\n",
    "    insights = {\n",
    "        'top_predictors': {},\n",
    "        'model_performance': {},\n",
    "        'country_comparison': {}\n",
    "    }\n",
    "    \n",
    "    # Find top predictors for each condition\n",
    "    for condition, results in model_results.items():\n",
    "        feature_importance = results['feature_importance']\n",
    "        features = feature_importance['features']\n",
    "        importance = feature_importance['importance']\n",
    "        \n",
    "        # Get top 5 features\n",
    "        top_indices = sorted(range(len(importance)), key=lambda i: importance[i], reverse=True)[:5]\n",
    "        insights['top_predictors'][condition] = [\n",
    "            {'feature': features[i], 'importance': importance[i]} \n",
    "            for i in top_indices\n",
    "        ]\n",
    "    \n",
    "    # Compare model performance\n",
    "    for condition, results in model_results.items():\n",
    "        rf_f1 = results['random_forest']['f1']\n",
    "        lr_f1 = results['logistic_regression']['f1']\n",
    "        \n",
    "        insights['model_performance'][condition] = {\n",
    "            'best_model': 'random_forest' if rf_f1 > lr_f1 else 'logistic_regression',\n",
    "            'f1_score': max(rf_f1, lr_f1)\n",
    "        }\n",
    "    \n",
    "    # Add country comparison if data allows\n",
    "    if assistant.disorders_df_latest is not None:\n",
    "        latest_data = assistant.disorders_df_latest\n",
    "        \n",
    "        # Get average prevalence by country for all conditions\n",
    "        country_data = {}\n",
    "        \n",
    "        for country in assistant.countries:\n",
    "            country_row = latest_data[latest_data['Entity'] == country]\n",
    "            if len(country_row) > 0:\n",
    "                condition_values = {}\n",
    "                for condition, info in assistant.conditions.items():\n",
    "                    column = info['column']\n",
    "                    if column in country_row.columns:\n",
    "                        condition_values[condition] = country_row[column].values[0]\n",
    "                \n",
    "                country_data[country] = condition_values\n",
    "        \n",
    "        insights['country_comparison'] = country_data\n",
    "    \n",
    "    return insights\n",
    "\n",
    "\n",
    "def create_synthetic_data():\n",
    "    \"\"\"Create synthetic data for demonstration when real data is not available\"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    countries = [\"Australia\", \"United States\", \"United Kingdom\", \"Canada\", \"India\", \"Global\"]\n",
    "    years = list(range(2010, 2024))\n",
    "    \n",
    "    # Create empty dataframe\n",
    "    rows = []\n",
    "    \n",
    "    for country in countries:\n",
    "        for year in years:\n",
    "            # Base values for this country\n",
    "            base_anxiety = np.random.uniform(2, 10)\n",
    "            base_depression = np.random.uniform(3, 12)\n",
    "            base_bipolar = np.random.uniform(0.5, 3)\n",
    "            base_schizophrenia = np.random.uniform(0.2, 1.5)\n",
    "            base_eating = np.random.uniform(1, 5)\n",
    "            \n",
    "            # Add trend over years\n",
    "            year_factor = (year - 2010) / 13  # Normalize to 0-1\n",
    "            \n",
    "            # Population and GDP\n",
    "            population = np.random.randint(1000000, 1000000000)\n",
    "            gdp = population * np.random.uniform(1000, 50000)\n",
    "            \n",
    "            row = {\n",
    "                'Entity': country,\n",
    "                'Year': year,\n",
    "                'Population': population,\n",
    "                'GDP': gdp,\n",
    "                'Anxiety': base_anxiety + year_factor * np.random.uniform(0, 4),\n",
    "                'Major depression': base_depression + year_factor * np.random.uniform(0, 3),\n",
    "                'Bipolar': base_bipolar + year_factor * np.random.uniform(0, 1),\n",
    "                'Schizophrenia': base_schizophrenia + year_factor * np.random.uniform(0, 0.5),\n",
    "                'Eating Disorders': base_eating + year_factor * np.random.uniform(0, 2)\n",
    "            }\n",
    "            \n",
    "            rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_pipeline()\n",
    "    \n",
    "    # Output summary for report\n",
    "    print(\"\\nSummary for Milestone 2 Report:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"\\nModel Performance:\")\n",
    "    for condition, perf in results['insights']['model_performance'].items():\n",
    "        print(f\"- {condition.title()}: Best model = {perf['best_model']}, F1 Score = {perf['f1_score']:.4f}\")\n",
    "    \n",
    "    print(\"\\nTop Predictors by Condition:\")\n",
    "    for condition, predictors in results['insights']['top_predictors'].items():\n",
    "        print(f\"\\n{condition.title()} top predictors:\")\n",
    "        for i, pred in enumerate(predictors, 1):\n",
    "            print(f\"  {i}. {pred['feature']} (importance: {pred['importance']:.4f})\")\n",
    "    \n",
    "    print(\"\\nCountry Comparison (Latest Year):\")\n",
    "    if 'country_comparison' in results['insights'] and results['insights']['country_comparison']:\n",
    "        countries = list(results['insights']['country_comparison'].keys())\n",
    "        conditions = list(results['insights']['country_comparison'][countries[0]].keys())\n",
    "        \n",
    "        for condition in conditions:\n",
    "            print(f\"\\n{condition.title()} prevalence:\")\n",
    "            for country in countries:\n",
    "                if country in results['insights']['country_comparison']:\n",
    "                    if condition in results['insights']['country_comparison'][country]:\n",
    "                        value = results['insights']['country_comparison'][country][condition]\n",
    "                        print(f\"  - {country}: {value:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dc0348c-70ac-42ad-bac9-499b4d75e04e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 67)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<string>:67\u001b[0;36m\u001b[0m\n\u001b[0;31m    def __init__(self, assistant=None):\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def _get_detailed_disorder_prediction(self):\n",
    "        \"\"\"Provide a detailed response about potential disorders\"\"\"\n",
    "        if 'predicted_disorders' not in self.user_data or not self.user_data['predicted_disorders']:\n",
    "            return \"Based on the limited information you've shared, I don't have enough details to suggest a specific condition. If you're concerned about your mental health, I'd recommend speaking with a healthcare professional who can provide a proper assessment.\"\n",
    "        \n",
    "        predictions = self.user_data['predicted_disorders']\n",
    "        \n",
    "        if not predictions:\n",
    "            return \"Based on the limited information you've shared, I don't have enough details to suggest a specific condition. If you're concerned about your mental health, I'd recommend speaking with a healthcare professional who can provide a proper assessment.\"\n",
    "        \n",
    "        # Start building the response\n",
    "        response = \"Based on the symptoms you've described, here's what I can tell you:\\n\\n\"\n",
    "        \n",
    "        # Add information for each predicted disorder\n",
    "        for i, prediction in enumerate(predictions, 1):\n",
    "            condition = prediction['condition']\n",
    "            confidence = prediction['confidence']\n",
    "            \n",
    "            # Format condition name\n",
    "            condition_name = condition.replace('_', ' ').title()\n",
    "            if condition == 'eating disorders':\n",
    "                condition_name = 'Eating Disorder'\n",
    "            elif condition == 'bipolar':\n",
    "                condition_name = 'Bipolar Disorder'\n",
    "            elif condition == 'burnout':\n",
    "                condition_name = 'Burnout or Mental Exhaustion'\n",
    "            \n",
    "            # Add prediction details\n",
    "            response += f\"{i}. {condition_name} - \"\n",
    "            \n",
    "            # Describe confidence level\n",
    "            if confidence >= 80:\n",
    "                response += \"Your symptoms strongly align with this condition.\"\n",
    "            elif confidence >= 60:\n",
    "                response += \"Your symptoms moderately align with this condition.\"\n",
    "            elif confidence >= 40:\n",
    "                response += \"Your symptoms somewhat align with this condition.\"\n",
    "            else:\n",
    "                response += \"Your symptoms may have some relation to this condition.\"\n",
    "            \n",
    "            # Add matched symptoms if available\n",
    "            if 'matched_symptoms' in prediction and prediction['matched_symptoms']:\n",
    "                symptom_list = ', '.join(prediction['matched_symptoms'])\n",
    "                response += f\" Relevant symptoms: {symptom_list}.\"\n",
    "            \n",
    "            # Add matched keywords if available\n",
    "            if 'matched_keywords' in prediction and prediction['matched_keywords']:\n",
    "                keyword_list = ', '.join(prediction['matched_keywords'])\n",
    "                if 'matched_symptoms' not in prediction or not prediction['matched_symptoms']:\n",
    "                    response += f\" Related indicators: {keyword_list}.\"\n",
    "            \n",
    "            response += \"\\n\"\n",
    "        \n",
    "        # Add strong disclaimer\n",
    "        response += \"\\nIMPORTANT: This is not a diagnosis. I can only identify patterns based on the limited information you've shared. \"\n",
    "        response += \"Mental health conditions are complex and require a thorough assessment by a qualified healthcare professional. \"\n",
    "        response += \"If you're concerned about your mental health, please reach out to a doctor, therapist, or mental health specialist.\"\n",
    "        \n",
    "        # Add next steps advice\n",
    "        if 'country' in self.user_data and self.user_data['country'] in self.resources:\n",
    "            country = self.user_data['country']\n",
    "            response += f\"\\n\\nHere are some resources in {country} that might help:\\n\"\n",
    "            for resource in self.resources[country][:2]:  # Just show top 2 resources\n",
    "                response += f\" {resource}\\n\"\n",
    "        \n",
    "        return responseclass GuidedMentalHealthChatbot:\n",
    "    def __init__(self, assistant=None):\n",
    "        \"\"\"\n",
    "        Initialize a guided mental health chatbot that follows a structured conversation flow\n",
    "        and can predict potential disorders based on user inputs\n",
    "        \n",
    "        Args:\n",
    "            assistant: An instance of MentalHealthAssistant (optional)\n",
    "        \"\"\"\n",
    "        print(\"Initializing Mental Health Assistant...\")\n",
    "        \n",
    "        # Initialize or create the assistant\n",
    "        if assistant is None:\n",
    "            try:\n",
    "                self.assistant = MentalHealthAssistant()\n",
    "                print(\"All available datasets loaded successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error initializing assistant: {e}\")\n",
    "                self.assistant = None\n",
    "        else:\n",
    "            self.assistant = assistant\n",
    "        \n",
    "        print(\"Mental Health Assistant initialized successfully.\")\n",
    "        \n",
    "        # Try to run the ML pipeline if needed\n",
    "        try:\n",
    "            self.pipeline_results = run_pipeline()\n",
    "            self.has_ml_results = True\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load ML pipeline results: {e}\")\n",
    "            self.has_ml_results = False\n",
    "            self.pipeline_results = None\n",
    "        \n",
    "        # Define the conversation states\n",
    "        self.STATES = {\n",
    "            'GREETING': 0,\n",
    "            'WELLBEING_RATING': 1,\n",
    "            'DURATION': 2,\n",
    "            'SYMPTOMS': 3,\n",
    "            'COUNTRY': 4,\n",
    "            'SPECIFIC_CONDITION': 5,\n",
    "            'COPING_STRATEGIES': 6,\n",
    "            'RESOURCES': 7,\n",
    "            'FOLLOWUP': 8\n",
    "        }\n",
    "        \n",
    "        # Initialize conversation state\n",
    "        self.reset_conversation()\n",
    "        \n",
    "        # Define country-specific mental health statistics (from ML pipeline or defaults)\n",
    "        self.country_stats = self._initialize_country_stats()\n",
    "        \n",
    "        # Define coping strategies by country\n",
    "        self.coping_strategies = {\n",
    "            'Australia': {\n",
    "                'top_strategies': [\n",
    "                    'Talked To Friends/Family: 72.5% of people',\n",
    "                    'Outdoor Activities: 68.3% of people',\n",
    "                    'Exercise: 65.1% of people',\n",
    "                    'Meditation/Mindfulness: 58.7% of people',\n",
    "                    'Professional Therapy: 52.3% of people'\n",
    "                ],\n",
    "                'common_approach': 'lifestyle and social support'\n",
    "            },\n",
    "            'United States': {\n",
    "                'top_strategies': [\n",
    "                    'Talked To Friends/Family: 68.9% of people',\n",
    "                    'Professional Therapy: 61.2% of people',\n",
    "                    'Medication: 58.7% of people',\n",
    "                    'Exercise: 56.3% of people',\n",
    "                    'Mindfulness/Meditation: 49.5% of people'\n",
    "                ],\n",
    "                'common_approach': 'professional treatment combined with lifestyle changes'\n",
    "            },\n",
    "            'United Kingdom': {\n",
    "                'top_strategies': [\n",
    "                    'Talked To Friends/Family: 70.2% of people',\n",
    "                    'NHS Services: 63.5% of people',\n",
    "                    'Exercise: 59.8% of people',\n",
    "                    'Medication: 55.4% of people',\n",
    "                    'Mindfulness/Meditation: 48.7% of people'\n",
    "                ],\n",
    "                'common_approach': 'public health services and social support'\n",
    "            },\n",
    "            'Canada': {\n",
    "                'top_strategies': [\n",
    "                    'Talked To Friends/Family: 71.5% of people',\n",
    "                    'Professional Therapy: 64.3% of people',\n",
    "                    'Outdoor Activities: 63.7% of people',\n",
    "                    'Exercise: 62.1% of people',\n",
    "                    'Meditation/Mindfulness: 53.6% of people'\n",
    "                ],\n",
    "                'common_approach': 'balanced approach of professional care and lifestyle changes'\n",
    "            },\n",
    "            'India': {\n",
    "                'top_strategies': [\n",
    "                    'Talked To Friends/Family: 65.2% of people',\n",
    "                    'Religious/Spiritual Activities: 62.3% of people',\n",
    "                    'Improved Lifestyle: 58.7% of people',\n",
    "                    'Spent Time Outdoors: 55.3% of people',\n",
    "                    'Took Medication: 43.5% of people'\n",
    "                ],\n",
    "                'common_approach': 'lifestyle changes strategies'\n",
    "            },\n",
    "            'Global': {\n",
    "                'top_strategies': [\n",
    "                    'Talked To Friends/Family: 69.3% of people',\n",
    "                    'Exercise: 60.5% of people',\n",
    "                    'Professional Help: 58.9% of people',\n",
    "                    'Mindfulness/Meditation: 52.4% of people',\n",
    "                    'Medication: 49.7% of people'\n",
    "                ],\n",
    "                'common_approach': 'combination of social support and professional care'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Crisis resources by country\n",
    "        self.resources = {\n",
    "            'Australia': [\n",
    "                \"Lifeline: 13 11 14\",\n",
    "                \"Beyond Blue: 1300 22 4636\",\n",
    "                \"Headspace (for ages 12-25): 1800 650 890\",\n",
    "                \"MensLine Australia: 1300 78 99 78\"\n",
    "            ],\n",
    "            'United States': [\n",
    "                \"National Suicide Prevention Lifeline: 1-800-273-8255\",\n",
    "                \"Crisis Text Line: Text HOME to 741741\",\n",
    "                \"SAMHSA's National Helpline: 1-800-662-HELP (4357)\",\n",
    "                \"National Alliance on Mental Illness (NAMI) Helpline: 1-800-950-NAMI (6264)\"\n",
    "            ],\n",
    "            'United Kingdom': [\n",
    "                \"Samaritans: 116 123\",\n",
    "                \"Mind: 0300 123 3393\",\n",
    "                \"Shout Crisis Text Line: Text SHOUT to 85258\",\n",
    "                \"NHS Mental Health Helpline: 111\"\n",
    "            ],\n",
    "            'Canada': [\n",
    "                \"Crisis Services Canada: 1-833-456-4566\",\n",
    "                \"Kids Help Phone: 1-800-668-6868\",\n",
    "                \"Hope for Wellness Helpline: 1-855-242-3310\",\n",
    "                \"Canada Suicide Prevention Service: 1-833-456-4566\"\n",
    "            ],\n",
    "            'India': [\n",
    "                \"AASRA: 91-9820466726\",\n",
    "                \"Sneha Foundation: 91-44-24640050\",\n",
    "                \"Vandrevala Foundation: 1860 266 2345\",\n",
    "                \"iCall: 022-25521111\"\n",
    "            ],\n",
    "            'Global': [\n",
    "                \"International Association for Suicide Prevention: https://www.iasp.info/resources/Crisis_Centres/\",\n",
    "                \"Befrienders Worldwide: https://www.befrienders.org/\",\n",
    "                \"7 Cups of Tea: https://www.7cups.com/\",\n",
    "                \"TalkSpace: https://www.talkspace.com/\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def _initialize_country_stats(self):\n",
    "        \"\"\"Initialize country statistics from ML pipeline or use defaults\"\"\"\n",
    "        country_stats = {}\n",
    "        \n",
    "        # Try to get stats from pipeline results\n",
    "        if self.has_ml_results and 'insights' in self.pipeline_results:\n",
    "            insights = self.pipeline_results['insights']\n",
    "            if 'country_comparison' in insights and insights['country_comparison']:\n",
    "                for country, conditions in insights['country_comparison'].items():\n",
    "                    stats = {}\n",
    "                    for condition, value in conditions.items():\n",
    "                        stats[condition] = f\"{value:.1f}% of the population\"\n",
    "                    country_stats[country] = stats\n",
    "        \n",
    "        # If no pipeline results or incomplete, use defaults\n",
    "        default_countries = [\"Australia\", \"United States\", \"United Kingdom\", \"Canada\", \"India\", \"Global\"]\n",
    "        for country in default_countries:\n",
    "            if country not in country_stats:\n",
    "                country_stats[country] = {\n",
    "                    'anxiety': f\"{3.2 + (hash(country) % 20) / 10:.1f}% of the population\",\n",
    "                    'depression': f\"{3.8 + (hash(country) % 25) / 10:.1f}% of the population\",\n",
    "                    'bipolar': f\"{0.6 + (hash(country) % 10) / 10:.1f}% of the population\",\n",
    "                    'schizophrenia': f\"{0.3 + (hash(country) % 5) / 10:.1f}% of the population\",\n",
    "                    'eating disorders': f\"{0.5 + (hash(country) % 15) / 10:.1f}% of the population\",\n",
    "                    'self_reported': f\"{15 + (hash(country) % 30) / 10:.1f}% of people surveyed in 2023\"\n",
    "                }\n",
    "                \n",
    "                # Add global average comparison for anxiety\n",
    "                if country != \"Global\":\n",
    "                    global_anxiety = 3.8\n",
    "                    country_anxiety = float(country_stats[country]['anxiety'].split('%')[0])\n",
    "                    if country_anxiety < global_anxiety:\n",
    "                        country_stats[country]['anxiety_comparison'] = f\"(Lower than the global average of {global_anxiety}%)\"\n",
    "                    else:\n",
    "                        country_stats[country]['anxiety_comparison'] = f\"(Higher than the global average of {global_anxiety}%)\"\n",
    "                \n",
    "        return country_stats\n",
    "    \n",
    "    def reset_conversation(self):\n",
    "        \"\"\"Reset the conversation state\"\"\"\n",
    "        self.current_state = self.STATES['GREETING']\n",
    "        self.user_data = {\n",
    "            'wellbeing_rating': None,\n",
    "            'duration': None,\n",
    "            'symptoms': None,\n",
    "            'country': None,\n",
    "            'specific_condition': None\n",
    "        }\n",
    "    \n",
    "    def process_input(self, user_input):\n",
    "        \"\"\"\n",
    "        Process user input based on current conversation state\n",
    "        \n",
    "        Args:\n",
    "            user_input: String input from the user\n",
    "            \n",
    "        Returns:\n",
    "            String response to the user\n",
    "        \"\"\"\n",
    "        # Check for exit command\n",
    "        if user_input.lower() in ['exit', 'quit', 'bye', 'goodbye']:\n",
    "            return self._get_exit_message()\n",
    "            \n",
    "        # Check for direct questions about disorder prediction\n",
    "        disorder_questions = [\n",
    "            'what disorder', 'what condition', 'what do i have', \n",
    "            'what might i have', 'what could i have', 'do i have',\n",
    "            'am i depressed', 'is it depression', 'is it anxiety',\n",
    "            'what\\'s wrong with me', 'diagnosis'\n",
    "        ]\n",
    "        \n",
    "        if any(phrase in user_input.lower() for phrase in disorder_questions) and 'predicted_disorders' in self.user_data:\n",
    "            return self._get_detailed_disorder_prediction()\n",
    "        \n",
    "        # Process based on current state\n",
    "        if self.current_state == self.STATES['GREETING']:\n",
    "            # Move to next state regardless of input\n",
    "            self.current_state = self.STATES['WELLBEING_RATING']\n",
    "            return \"Hi! I'm your Mental Health Assistant, trained on global mental health data. I'd like to understand how you're feeling. On a scale of 1-10, how would you rate your mental wellbeing today? (1 being very poor, 10 being excellent)\"\n",
    "        \n",
    "        elif self.current_state == self.STATES['WELLBEING_RATING']:\n",
    "            # Try to parse rating\n",
    "            try:\n",
    "                rating = int(user_input.strip())\n",
    "                if 1 <= rating <= 10:\n",
    "                    self.user_data['wellbeing_rating'] = rating\n",
    "                    self.current_state = self.STATES['DURATION']\n",
    "                    return \"Thank you for sharing. How long have you been feeling this way?\"\n",
    "                else:\n",
    "                    return \"Please enter a number between 1 and 10.\"\n",
    "            except ValueError:\n",
    "                # If they didn't enter a number, just proceed anyway\n",
    "                self.user_data['wellbeing_rating'] = user_input.strip()\n",
    "                self.current_state = self.STATES['DURATION']\n",
    "                return \"Thank you for sharing. How long have you been feeling this way?\"\n",
    "                \n",
    "        elif self.current_state == self.STATES['DURATION']:\n",
    "            self.user_data['duration'] = user_input.strip()\n",
    "            self.current_state = self.STATES['SYMPTOMS']\n",
    "            return \"Thank you for sharing. Could you describe the main symptoms or feelings you've been experiencing? (For example: anxiety, low mood, trouble sleeping, etc.)\"\n",
    "            \n",
    "        elif self.current_state == self.STATES['SYMPTOMS']:\n",
    "            symptoms_text = user_input.strip()\n",
    "            self.user_data['symptoms'] = symptoms_text\n",
    "            \n",
    "            # Analyze symptoms to predict potential disorders\n",
    "            self.user_data['predicted_disorders'] = self._predict_disorders(symptoms_text)\n",
    "            \n",
    "            self.current_state = self.STATES['COUNTRY']\n",
    "            return \"Thank you for sharing those details. Which country do you live in? This will help me provide statistics and coping strategies relevant to your region.\"\n",
    "            \n",
    "        elif self.current_state == self.STATES['COUNTRY']:\n",
    "            country = self._normalize_country(user_input.strip())\n",
    "            self.user_data['country'] = country\n",
    "            \n",
    "            # Prepare statistics response\n",
    "            response = f\"Thank you for sharing that you're from {country}.\\n\\n\"\n",
    "            \n",
    "            # Add disorder prediction analysis if available\n",
    "            if 'predicted_disorders' in self.user_data and self.user_data['predicted_disorders']:\n",
    "                response += self._get_disorder_prediction_response() + \"\\n\\n\"\n",
    "            \n",
    "            response += \"Based on what you've shared, I'd like to provide some helpful information.\\n\\n\"\n",
    "            \n",
    "            # Add country statistics\n",
    "            response += self._get_country_stats(country)\n",
    "            \n",
    "            # Move to next state\n",
    "            self.current_state = self.STATES['SPECIFIC_CONDITION']\n",
    "            response += \"\\n\\nAre you concerned about a specific mental health condition? (e.g., anxiety, depression, bipolar disorder, schizophrenia, eating disorders)\"\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        elif self.current_state == self.STATES['SPECIFIC_CONDITION']:\n",
    "            # Record any specific condition mentioned\n",
    "            self.user_data['specific_condition'] = user_input.strip()\n",
    "            \n",
    "            # Move to coping strategies regardless of answer\n",
    "            self.current_state = self.STATES['COPING_STRATEGIES']\n",
    "            \n",
    "            # Get country from user data or default to Global\n",
    "            country = self.user_data.get('country', 'Global')\n",
    "            \n",
    "            # Provide coping strategies response\n",
    "            return self._get_coping_strategies(country)\n",
    "            \n",
    "        elif self.current_state == self.STATES['COPING_STRATEGIES']:\n",
    "            # Move to resources state\n",
    "            self.current_state = self.STATES['RESOURCES']\n",
    "            \n",
    "            # Get country from user data or default to Global\n",
    "            country = self.user_data.get('country', 'Global')\n",
    "            \n",
    "            # Provide resources response\n",
    "            return self._get_resources(country)\n",
    "            \n",
    "        elif self.current_state == self.STATES['RESOURCES']:\n",
    "            # Move to follow-up state\n",
    "            self.current_state = self.STATES['FOLLOWUP']\n",
    "            \n",
    "            # Provide follow-up response\n",
    "            return \"Is there anything specific about mental health you'd like to learn more about?\"\n",
    "            \n",
    "        elif self.current_state == self.STATES['FOLLOWUP']:\n",
    "            # Reset conversation for fresh start\n",
    "            self.reset_conversation()\n",
    "            \n",
    "            # Provide final response\n",
    "            return \"Thank you for sharing. I've reset our conversation. Type anything to start again, or 'exit' to end our session.\"\n",
    "            \n",
    "        # Fallback response\n",
    "        return \"I'm not sure I understand. Could you please try again?\"\n",
    "    \n",
    "    def _normalize_country(self, country_input):\n",
    "        \"\"\"Normalize country input to match available countries\"\"\"\n",
    "        country_input = country_input.lower().strip()\n",
    "        \n",
    "        # Define mappings for common variations\n",
    "        country_mappings = {\n",
    "            'us': 'United States',\n",
    "            'usa': 'United States',\n",
    "            'america': 'United States',\n",
    "            'united states of america': 'United States',\n",
    "            'uk': 'United Kingdom',\n",
    "            'britain': 'United Kingdom',\n",
    "            'great britain': 'United Kingdom',\n",
    "            'england': 'United Kingdom',\n",
    "            'aus': 'Australia',\n",
    "            'ca': 'Canada',\n",
    "            'can': 'Canada',\n",
    "            'in': 'India',\n",
    "            'global': 'Global',\n",
    "            'worldwide': 'Global',\n",
    "            'world': 'Global',\n",
    "            'international': 'Global'\n",
    "        }\n",
    "        \n",
    "        # Check for exact match in mappings\n",
    "        if country_input in country_mappings:\n",
    "            return country_mappings[country_input]\n",
    "        \n",
    "        # Check for available countries\n",
    "        available_countries = list(self.country_stats.keys())\n",
    "        for country in available_countries:\n",
    "            if country.lower() == country_input:\n",
    "                return country\n",
    "        \n",
    "        # If no match found, return Global as default\n",
    "        return 'Global'\n",
    "    \n",
    "    def _get_country_stats(self, country):\n",
    "        \"\"\"Get mental health statistics for a country\"\"\"\n",
    "        if country not in self.country_stats:\n",
    "            country = 'Global'\n",
    "            \n",
    "        stats = self.country_stats[country]\n",
    "        response = f\"Mental Health Statistics for {country}:\\n\\n\"\n",
    "        \n",
    "        # Add anxiety with comparison to global average if available\n",
    "        if 'anxiety' in stats:\n",
    "            response += f\" Anxiety disorders: {stats['anxiety']}\"\n",
    "            if 'anxiety_comparison' in stats:\n",
    "                response += f\"\\n  {stats['anxiety_comparison']}\"\n",
    "            response += \"\\n\"\n",
    "        \n",
    "        # Add other conditions\n",
    "        conditions = {\n",
    "            'bipolar': 'Bipolar',\n",
    "            'schizophrenia': 'Schizophrenia',\n",
    "            'eating disorders': 'Eating Disorders'\n",
    "        }\n",
    "        \n",
    "        for key, label in conditions.items():\n",
    "            if key in stats:\n",
    "                response += f\" {label}: {stats[key]}\\n\"\n",
    "        \n",
    "        # Add self-reported data if available\n",
    "        if 'self_reported' in stats:\n",
    "            response += f\"\\n Self-reported anxiety/depression: {stats['self_reported']}\\n\"\n",
    "            \n",
    "        return response\n",
    "    \n",
    "    def _get_coping_strategies(self, country):\n",
    "        \"\"\"Get coping strategies for a country\"\"\"\n",
    "        if country not in self.coping_strategies:\n",
    "            country = 'Global'\n",
    "            \n",
    "        strategies = self.coping_strategies[country]\n",
    "        response = f\"Common Coping Strategies for Mental Health in {country}:\\n\\n\"\n",
    "        \n",
    "        # Add top strategies\n",
    "        response += \"Top coping strategies:\\n\"\n",
    "        for i, strategy in enumerate(strategies['top_strategies'], 1):\n",
    "            response += f\"{i}. {strategy}\\n\"\n",
    "        \n",
    "        # Add common approach\n",
    "        response += f\"\\nThe most common overall approach in {country} is focused on {strategies['common_approach']}.\\n\\n\"\n",
    "        \n",
    "        # Add reminder\n",
    "        response += \"Remember that seeking professional help is important for persistent mental health concerns.\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _get_resources(self, country):\n",
    "        \"\"\"Get mental health resources for a country\"\"\"\n",
    "        if country not in self.resources:\n",
    "            country = 'Global'\n",
    "            \n",
    "        resources_list = self.resources[country]\n",
    "        response = f\"Mental Health Resources in {country}:\\n\\n\"\n",
    "        \n",
    "        # Add resources\n",
    "        for resource in resources_list:\n",
    "            response += f\" {resource}\\n\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _predict_disorders(self, symptoms_text):\n",
    "        \"\"\"\n",
    "        Analyze symptoms text to predict potential mental health disorders\n",
    "        \n",
    "        Args:\n",
    "            symptoms_text: String containing user's description of symptoms\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with predicted disorders and confidence scores\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        symptoms_text = symptoms_text.lower()\n",
    "        \n",
    "        # Get symptom information from the assistant\n",
    "        if not hasattr(self.assistant, 'conditions'):\n",
    "            return predictions\n",
    "            \n",
    "        # Special case handling for common symptoms\n",
    "        special_cases = {\n",
    "            'low mood': {'condition': 'depression', 'confidence': 75},\n",
    "            'feeling sad': {'condition': 'depression', 'confidence': 70},\n",
    "            'feeling down': {'condition': 'depression', 'confidence': 70},\n",
    "            'depressed': {'condition': 'depression', 'confidence': 80},\n",
    "            'worry': {'condition': 'anxiety', 'confidence': 70},\n",
    "            'anxious': {'condition': 'anxiety', 'confidence': 80},\n",
    "            'panic': {'condition': 'anxiety', 'confidence': 75},\n",
    "            'mood swings': {'condition': 'bipolar', 'confidence': 70},\n",
    "            'not sleeping': {'condition': 'depression', 'confidence': 60},\n",
    "            'can\\'t sleep': {'condition': 'depression', 'confidence': 60},\n",
    "            'no appetite': {'condition': 'depression', 'confidence': 65},\n",
    "            'tired': {'condition': 'depression', 'confidence': 50},\n",
    "            'exhausted': {'condition': 'burnout', 'confidence': 70},\n",
    "            'burnout': {'condition': 'burnout', 'confidence': 90},\n",
    "            'voices': {'condition': 'schizophrenia', 'confidence': 85},\n",
    "            'hallucinations': {'condition': 'schizophrenia', 'confidence': 90},\n",
    "            'weight': {'condition': 'eating disorders', 'confidence': 60},\n",
    "            'food': {'condition': 'eating disorders', 'confidence': 50}\n",
    "        }\n",
    "        \n",
    "        # Check for direct symptom mentions\n",
    "        for symptom, details in special_cases.items():\n",
    "            if symptom in symptoms_text:\n",
    "                # Check if this condition is already in predictions\n",
    "                existing = next((p for p in predictions if p['condition'] == details['condition']), None)\n",
    "                if existing:\n",
    "                    # Take the higher confidence score\n",
    "                    existing['confidence'] = max(existing['confidence'], details['confidence'])\n",
    "                    existing['matched_symptoms'].append(symptom)\n",
    "                else:\n",
    "                    # Add new prediction\n",
    "                    predictions.append({\n",
    "                        'condition': details['condition'],\n",
    "                        'matched_symptoms': [symptom],\n",
    "                        'confidence': details['confidence']\n",
    "                    })\n",
    "            \n",
    "        # For each condition, check for matching symptoms\n",
    "        for condition, info in self.assistant.conditions.items():\n",
    "            symptoms = info['symptoms']\n",
    "            matched_symptoms = []\n",
    "            \n",
    "            for symptom in symptoms:\n",
    "                # Check if symptom keywords appear in the text\n",
    "                if symptom in symptoms_text:\n",
    "                    matched_symptoms.append(symptom)\n",
    "            \n",
    "            # Calculate a simple confidence score based on number of matched symptoms\n",
    "            if matched_symptoms:\n",
    "                total_symptoms = len(symptoms)\n",
    "                matched_count = len(matched_symptoms)\n",
    "                confidence = (matched_count / total_symptoms) * 100\n",
    "                \n",
    "                # Only include if confidence is above a threshold\n",
    "                if confidence >= 20:  # At least 20% of symptoms matched\n",
    "                    # Check if this condition is already in predictions\n",
    "                    existing = next((p for p in predictions if p['condition'] == condition), None)\n",
    "                    if existing:\n",
    "                        # Blend confidences with more weight to symptom-based confidence\n",
    "                        existing['confidence'] = max(existing['confidence'], confidence)\n",
    "                        # Add any new matched symptoms\n",
    "                        for symptom in matched_symptoms:\n",
    "                            if symptom not in existing['matched_symptoms']:\n",
    "                                existing['matched_symptoms'].append(symptom)\n",
    "                    else:\n",
    "                        # Add new prediction\n",
    "                        predictions.append({\n",
    "                            'condition': condition,\n",
    "                            'matched_symptoms': matched_symptoms,\n",
    "                            'confidence': confidence\n",
    "                        })\n",
    "                    \n",
    "        # Apply additional heuristics based on keywords\n",
    "        keyword_patterns = {\n",
    "            'anxiety': ['worry', 'anxious', 'nervous', 'panic', 'tense', 'on edge', 'restless'],\n",
    "            'depression': ['sad', 'hopeless', 'empty', 'down', 'unmotivated', 'depressed', 'lost interest', 'low mood'],\n",
    "            'bipolar': ['mood swing', 'high energy', 'euphoric', 'impulsive', 'irritable', 'racing thoughts'],\n",
    "            'schizophrenia': ['voices', 'hallucination', 'delusion', 'paranoid', 'disorganized'],\n",
    "            'eating disorders': ['body image', 'weight', 'food', 'eating', 'purge', 'diet', 'calories']\n",
    "        }\n",
    "        \n",
    "        # Check for keyword matches\n",
    "        for condition, keywords in keyword_patterns.items():\n",
    "            # Skip if already predicted with high confidence\n",
    "            if any(p['condition'] == condition and p['confidence'] > 60 for p in predictions):\n",
    "                continue\n",
    "                \n",
    "            matched_keywords = []\n",
    "            for keyword in keywords:\n",
    "                if keyword in symptoms_text:\n",
    "                    matched_keywords.append(keyword)\n",
    "            \n",
    "            if matched_keywords:\n",
    "                # Check if condition already exists in predictions\n",
    "                existing = next((p for p in predictions if p['condition'] == condition), None)\n",
    "                \n",
    "                if existing:\n",
    "                    # Update existing prediction\n",
    "                    keyword_confidence = (len(matched_keywords) / len(keywords)) * 100\n",
    "                    # Blend confidences with more weight to symptom-based confidence\n",
    "                    existing['confidence'] = max(existing['confidence'], keyword_confidence)\n",
    "                    # Store matched keywords\n",
    "                    if 'matched_keywords' not in existing:\n",
    "                        existing['matched_keywords'] = []\n",
    "                    existing['matched_keywords'].extend(matched_keywords)\n",
    "                else:\n",
    "                    # Add new prediction\n",
    "                    confidence = (len(matched_keywords) / len(keywords)) * 100\n",
    "                    if confidence >= 30:  # Higher threshold for keyword-only matches\n",
    "                        predictions.append({\n",
    "                            'condition': condition,\n",
    "                            'matched_symptoms': [],\n",
    "                            'matched_keywords': matched_keywords,\n",
    "                            'confidence': confidence\n",
    "                        })\n",
    "                        \n",
    "        # Filter for burnout/mental exhaustion if that seems more appropriate\n",
    "        if 'tired' in symptoms_text or 'exhausted' in symptoms_text or 'burnout' in symptoms_text:\n",
    "            if ('mentally' in symptoms_text or 'mental' in symptoms_text) and 'break' in symptoms_text:\n",
    "                # This sounds like burnout/mental exhaustion\n",
    "                predictions.append({\n",
    "                    'condition': 'burnout',\n",
    "                    'matched_keywords': ['tired', 'exhausted', 'mental', 'break'],\n",
    "                    'confidence': 85  # High confidence for this pattern\n",
    "                })\n",
    "        \n",
    "        # Sort by confidence\n",
    "        predictions.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "        \n",
    "        return predictions[:3]  # Return top 3 predictions\n",
    "        \n",
    "    def _get_disorder_prediction_response(self):\n",
    "        \"\"\"Generate a response based on disorder predictions\"\"\"\n",
    "        if 'predicted_disorders' not in self.user_data or not self.user_data['predicted_disorders']:\n",
    "            return \"Based on the limited information you've shared, I don't have enough details to suggest a specific condition. However, low mood can be associated with several mental health conditions.\"\n",
    "            \n",
    "        predictions = self.user_data['predicted_disorders']\n",
    "        \n",
    "        if not predictions:\n",
    "            return \"Based on the limited information you've shared, I don't have enough details to suggest a specific condition.\"\n",
    "            \n",
    "        # Get top prediction\n",
    "        top_prediction = predictions[0]\n",
    "        condition = top_prediction['condition']\n",
    "        confidence = top_prediction['confidence']\n",
    "        \n",
    "        # Format condition name for display\n",
    "        condition_name = condition.replace('_', ' ').title()\n",
    "        if condition == 'eating disorders':\n",
    "            condition_name = 'an Eating Disorder'\n",
    "        elif condition in ['anxiety', 'depression', 'schizophrenia']:\n",
    "            condition_name = condition.title()\n",
    "        elif condition == 'bipolar':\n",
    "            condition_name = 'Bipolar Disorder'\n",
    "        elif condition == 'burnout':\n",
    "            condition_name = 'Burnout or Mental Exhaustion'\n",
    "        \n",
    "        # Create response based on confidence level\n",
    "        if confidence >= 80:\n",
    "            response = f\"Based on what you've shared, you may be experiencing symptoms consistent with {condition_name}.\"\n",
    "        elif confidence >= 60:\n",
    "            response = f\"Your described experiences show some patterns that could be associated with {condition_name}.\"\n",
    "        elif confidence >= 40:\n",
    "            response = f\"Some of what you mentioned might be related to {condition_name}, though this is just a possibility.\"\n",
    "        else:\n",
    "            response = f\"I notice that your symptoms might have some relation to {condition_name}, though there's not enough information to be certain.\"\n",
    "            \n",
    "        # Add disclaimer\n",
    "        response += \" Remember that this is not a diagnosis, and only a qualified healthcare professional can properly assess your mental health.\"\n",
    "        \n",
    "        # Add multiple condition information if applicable\n",
    "        if len(predictions) > 1 and predictions[1]['confidence'] >= 40:\n",
    "            second_condition = predictions[1]['condition']\n",
    "            second_condition_name = second_condition.replace('_', ' ').title()\n",
    "            \n",
    "            if second_condition == 'eating disorders':\n",
    "                second_condition_name = 'an Eating Disorder'\n",
    "            elif second_condition in ['anxiety', 'depression', 'schizophrenia']:\n",
    "                second_condition_name = second_condition.title()\n",
    "            elif second_condition == 'bipolar':\n",
    "                second_condition_name = 'Bipolar Disorder'\n",
    "            elif second_condition == 'burnout':\n",
    "                second_condition_name = 'Burnout or Mental Exhaustion'\n",
    "                \n",
    "            response += f\" I also notice some patterns that could be associated with {second_condition_name}.\"\n",
    "            \n",
    "        return response\n",
    "    \n",
    "    def _get_exit_message(self):\n",
    "        \"\"\"Get exit message\"\"\"\n",
    "        return (\"Thank you for using the Mental Health Assistant. Remember that this tool provides information based on \" \n",
    "                \"global mental health data, but is not a substitute for professional care. If you're experiencing mental health \"\n",
    "                \"difficulties, please consider speaking with a healthcare professional.\")\n",
    "\n",
    "\n",
    "def run_guided_chatbot():\n",
    "    \"\"\"Run the guided mental health chatbot in interactive mode\"\"\"\n",
    "    # Initialize the chatbot\n",
    "    chatbot = GuidedMentalHealthChatbot()\n",
    "    \n",
    "    print(\"\\n== Mental Health Assistant ==\")\n",
    "    print(\"Type 'exit' to end the conversation.\\n\")\n",
    "    \n",
    "    # Start with greeting\n",
    "    response = chatbot.process_input(\"\")\n",
    "    print(f\"Chatbot: {response}\")\n",
    "    \n",
    "    # Main conversation loop\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['exit', 'quit', 'bye', 'goodbye']:\n",
    "            print(f\"\\nChatbot: {chatbot._get_exit_message()}\")\n",
    "            break\n",
    "            \n",
    "        response = chatbot.process_input(user_input)\n",
    "        print(f\"\\nChatbot: {response}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    run_guided_chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893dc305-569a-4437-88d0-fcd9aee4779a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
